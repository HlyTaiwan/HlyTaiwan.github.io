{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcAvwTAPbl-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44ca0f9-757b-4c6e-ecb2-4734ef45e7ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting user_agent\n",
            "  Downloading user_agent-0.1.10.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from user_agent) (1.16.0)\n",
            "Building wheels for collected packages: user_agent\n",
            "  Building wheel for user_agent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for user_agent: filename=user_agent-0.1.10-py3-none-any.whl size=18966 sha256=aea1971be807ffd7f832d29f95d6a150815a444bad2f877567cca30065cb522a\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/29/26/1956a891a058037774285ee79ab5c3ecf034dba50a4198fedd\n",
            "Successfully built user_agent\n",
            "Installing collected packages: user_agent\n",
            "Successfully installed user_agent-0.1.10\n",
            "Collecting urlopen\n",
            "  Downloading urlopen-1.0.0.zip (2.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: urlopen\n",
            "  Building wheel for urlopen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for urlopen: filename=urlopen-1.0.0-py3-none-any.whl size=1384 sha256=794e962b43a486a882b85cedcb6e861bba51938722a55bed0a074df4f4478103\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/48/27/6bdee97affb7718f3ad2215a99a8066a0e40e07000e0afda54\n",
            "Successfully built urlopen\n",
            "Installing collected packages: urlopen\n",
            "Successfully installed urlopen-1.0.0\n",
            "Collecting pip-system-certs\n",
            "  Downloading pip_system_certs-4.0-py2.py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: wrapt>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from pip-system-certs) (1.14.1)\n",
            "Installing collected packages: pip-system-certs\n",
            "Successfully installed pip-system-certs-4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install user_agent\n",
        "!pip install urlopen\n",
        "!pip install pip-system-certs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from user_agent import generate_user_agent\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8c22dPgDcEpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crawler(url):\n",
        "    time.sleep(1)\n",
        "    # 製作對網頁請求的假資料，如：假裝是IOS手機、Android手機、mac瀏覽器、desktop\n",
        "    user_agent = generate_user_agent(device_type='desktop')\n",
        "    # 代理 ip\n",
        "    # proxies = {\n",
        "    #   'http': '123.205.24.240:80',\n",
        "    # }\n",
        "    # headers\n",
        "    headers={\n",
        "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
        "        'accept-encoding': 'gzip, deflate, br',\n",
        "        'accept-language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,',\n",
        "        'sec-ch-ua': '\"Google Chrome\";v=\"111\", \"Not(A:Brand\";v=\"8\", \"Chromium\";v=\"111\"',\n",
        "        'sec-ch-ua-mobile': '?0',\n",
        "        'sec-ch-ua-platform': 'macOS',\n",
        "        'sec-fetch-dest': 'document',\n",
        "        'sec-fetch-mode': 'navigate',\n",
        "        'sec-fetch-site': 'same-origin',\n",
        "        'sec-fetch-user': '?1',\n",
        "        'upgrade-insecure-requests': '1',\n",
        "        'user-agent': 'Safari/537.36',\n",
        "    }\n",
        "    #對指定的url連結做請求，並給予假資料\n",
        "    r = requests.get(url, headers=headers)\n",
        "    #r = requests.get(url, headers=headers, allow_redirects=False)\n",
        "    r.encoding='utf-8'\n",
        "    #將網頁作解析，以利找到所需資料\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    #回傳整個網頁的解析\n",
        "    return soup"
      ],
      "metadata": {
        "id": "ZcysGnlTcIIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 棒球資料整理"
      ],
      "metadata": {
        "id": "reMa828x6Td_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_box_score, date, away_team, win_team = [], [], [], []\n",
        "with open('2023樂天.txt', 'r', encoding=\"utf-8\") as file:\n",
        "  soup = BeautifulSoup(file, 'html.parser')\n",
        "  tr = soup.find('div', class_ = 'RecordTable').find_all('tr')\n",
        "  for td in tr[1:]:\n",
        "    for_url = td.find('td', class_ = 'sticky num')\n",
        "    href = str(for_url.find('a')).split('\"')[1].replace('amp;', '')\n",
        "    for_url_str = f\"https://www.cpbl.com.tw{href}\"\n",
        "    url_box_score.append(for_url_str)\n",
        "\n",
        "    for_date = td.find_all('td', class_ = 'num')[1].text\n",
        "    date.append(for_date)\n",
        "\n",
        "    for_away = td.find_all('td')[4].text\n",
        "    away_team.append(for_away)\n",
        "\n",
        "    for_wlt = td.find_all('td')[8].text\n",
        "    win_team.append(for_wlt)\n",
        "\n",
        "with open('2022樂天.txt', 'r', encoding=\"utf-8\") as file:\n",
        "  soup = BeautifulSoup(file, 'html.parser')\n",
        "  tr = soup.find('div', class_ = 'RecordTable').find_all('tr')\n",
        "  for td in tr[1:]:\n",
        "    for_url = td.find('td', class_ = 'sticky num')\n",
        "    href = str(for_url.find('a')).split('\"')[1].replace('amp;', '')\n",
        "    for_url_str = f\"https://www.cpbl.com.tw{href}\"\n",
        "    url_box_score.append(for_url_str)\n",
        "\n",
        "    for_date = td.find_all('td', class_ = 'num')[1].text\n",
        "    date.append(for_date)\n",
        "\n",
        "    for_away = td.find_all('td')[4].text\n",
        "    away_team.append(for_away)\n",
        "\n",
        "    for_wlt = td.find_all('td')[8].text\n",
        "    win_team.append(for_wlt)\n"
      ],
      "metadata": {
        "id": "b0iI0QO7XgM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "win_lose = pd.read_csv(io.StringIO('''\n",
        "31W-28L\n",
        "31W-27L\n",
        "30W-27L\n",
        "28W-25L\n",
        "26W-24L\n",
        "25W-24L\n",
        "25W-23L\n",
        "23W-21L\n",
        "23W-20L\n",
        "19W-17L\n",
        "18W-17L\n",
        "18W-16L\n",
        "18W-14L\n",
        "16W-14L\n",
        "16W-13L\n",
        "16W-12L\n",
        "16W-11L\n",
        "15W-11L\n",
        "14W-11L\n",
        "13W-11L\n",
        "12W-11L\n",
        "10W-9L\n",
        "10W-8L\n",
        "10W-7L\n",
        "9W-7L\n",
        "7W-6L\n",
        "6W-6L\n",
        "3W-4L\n",
        "3W-3L\n",
        "2W-3L\n",
        "29W-28L\n",
        "28W-27L\n",
        "27W-27L\n",
        "26W-27L\n",
        "24W-26L\n",
        "23W-25L\n",
        "23W-25L\n",
        "29W-27L\n",
        "29W-27L\n",
        "29W-27L\n",
        "19W-16L\n",
        "19W-15L\n",
        "19W-14L\n",
        "19W-13L\n",
        "19W-12L\n",
        "19W-11L\n",
        "18W-11L\n",
        "17W-11L\n",
        "13W-9L\n",
        "13W-8L\n",
        "12W-8L\n",
        "11W-8L\n",
        "10W-7L\n",
        "9W-7L\n",
        "8W-7L\n",
        "8W-6L\n",
        "5W-4L\n",
        "5W-3L\n",
        "5W-2L\n",
        "1W-0L\n",
        "33W-24L\n",
        "33W-23L\n",
        "33W-22L\n",
        "33W-22L\n",
        "31W-20L\n",
        "28W-20L\n",
        "28W-19L\n",
        "27W-17L\n",
        "26W-14L\n",
        "25W-13L\n",
        "25W-12L\n",
        "25W-11L\n",
        "25W-10L\n",
        "24W-10L\n",
        "23W-10L\n",
        "22W-10L\n",
        "21W-10L\n",
        "20W-10L\n",
        "16W-7L\n",
        "16W-6L\n",
        "14W-4L\n",
        "14W-3L\n",
        "14W-3L\n",
        "14W-2L\n",
        "9W-1L\n",
        "8W-1L\n",
        "7W-1L\n",
        "4W-1L\n",
        "4W-0L\n",
        "3W-0L\n",
        "37W-22L\n",
        "36W-22L\n",
        "36W-21L\n",
        "35W-21L\n",
        "29W-19L\n",
        "29W-18L\n",
        "28W-18L\n",
        "28W-17L\n",
        "28W-16L\n",
        "27W-16L\n",
        "27W-15L\n",
        "22W-14L\n",
        "22W-13L\n",
        "20W-10L\n",
        "20W-9L\n",
        "20W-8L\n",
        "18W-6L\n",
        "17W-6L\n",
        "16W-6L\n",
        "15W-6L\n",
        "12W-5L\n",
        "12W-4L\n",
        "12W-4L\n",
        "11W-4L\n",
        "10W-3L\n",
        "10W-2L\n",
        "9W-2L\n",
        "4W-1L\n",
        "3W-1L\n",
        "1W-0L\n",
        "'''), header=None)\n",
        "win = [int(x.split('W')[0]) for x in win_lose[0].tolist()]\n",
        "lose = [int(x.split('-')[1].strip('L')) for x in win_lose[0].tolist()]\n",
        "\n",
        "for x in range(120):\n",
        "  if win_team[x] == '樂天桃猿':\n",
        "    win[x] = win[x] - 1\n",
        "  elif win_team [x] == '':\n",
        "    win[x] == win[x]\n",
        "  else:\n",
        "    lose[x] = lose[x] - 1\n",
        "\n",
        "win = np.array(win)\n",
        "lose = np.array(lose)\n",
        "win_rate = win / (win + lose)\n",
        "df = pd.DataFrame({\n",
        "    'date': date,\n",
        "    'win_rate': win_rate})\n",
        "df.to_csv('win_rate.csv', index = False)\n"
      ],
      "metadata": {
        "id": "IRERtUhuYm1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pitcher = pd.read_csv('2022-23_投手 - 2022_陳禹勳.csv')\n",
        "IP_list = []\n",
        "for row in df_pitcher['局數']:\n",
        "  ip = str(row)\n",
        "  if ip.find('.') != -1:\n",
        "    X = ip.split('.')\n",
        "    X = [int(x) for x in X]\n",
        "    IP = (X[1]/33 + X[0]*3)/ 3\n",
        "  else:\n",
        "    IP = row\n",
        "  IP_list.append(IP)\n",
        "\n",
        "ER = df_pitcher['責失'].tolist()\n",
        "ERA = []\n",
        "for x in range(907):\n",
        "  try:\n",
        "    era = ER[x] * 9 / IP_list[x]\n",
        "  except ZeroDivisionError:\n",
        "    era = float('inf')\n",
        "  ERA.append(era)\n",
        "\n",
        "df_pitcher['ERA'] = ERA\n",
        "df_pitcher['QS'] = (df_pitcher['ERA'] <= 4.5) & (df_pitcher['局數'] >= 6)\n",
        "df_pitcher['QS'] = df_pitcher['QS'].astype(int)\n",
        "\n",
        "df_pitcher.to_csv('2022-23_pitcher.csv', index = False)"
      ],
      "metadata": {
        "id": "wiJlSVoCDm6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batter = pd.read_csv('batter.csv')\n",
        "weekend_change = ['2023-10-09 00:00:00', '2023-09-29 00:00:00', '2023-06-22 00:00:00',\n",
        "                  '2022-10-10 00:00:00', '2022-04-05 00:00:00']\n",
        "batter = batter.dropna().reset_index(drop=True)\n",
        "for x in range(120):\n",
        "  if batter['date'][x] in weekend_change:\n",
        "    batter['weekend'][x] = 1\n",
        "\n",
        "  win_ = int(batter['w_l_t'][x].split('W')[0])\n",
        "  lose_ = int(batter['w_l_t'][x].split('-')[1].rstrip('L'))\n",
        "\n",
        "  if batter['win_team'][x] == '樂天桃猿':\n",
        "    win_ = win_ - 1\n",
        "  elif batter['win_team'][x] == '平手':\n",
        "    win_ = win_\n",
        "  else:\n",
        "    lose_ = lose_ - 1\n",
        "\n",
        "  if win_ + lose_ != 0:\n",
        "    batter['win_rate'][x] = win_ / (win_ + lose_)\n",
        "  else:\n",
        "    batter['win_rate'][x] = 0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vAo8VwJ68Fb",
        "outputId": "ef116287-4b4e-4d71-b824-aa76b685390f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-98f2172963b5>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  batter['win_rate'][x] = win_ / (win_ + lose_)\n",
            "<ipython-input-47-98f2172963b5>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  batter['weekend'][x] = 1\n",
            "<ipython-input-47-98f2172963b5>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  batter['win_rate'][x] = 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batter.to_csv('batter.csv', index = False)"
      ],
      "metadata": {
        "id": "Yk1lMbMHFAuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pitcher = pd.read_csv('pitcher.csv')\n",
        "weekend_change = ['2023-10-09 00:00:00', '2023-09-29 00:00:00', '2023-06-22 00:00:00',\n",
        "                  '2022-10-10 00:00:00', '2022-04-05 00:00:00']\n",
        "pitcher = pitcher.dropna(subset = 'weekend').reset_index(drop=True)\n",
        "for x in range(120):\n",
        "  if pitcher['date'][x] in weekend_change:\n",
        "    pitcher['weekend'][x] = 1\n",
        "\n",
        "  win_ = int(pitcher['w_l_t'][x].split('W')[0])\n",
        "  lose_ = int(pitcher['w_l_t'][x].split('-')[1].rstrip('L'))\n",
        "\n",
        "  if pitcher['win_team'][x] == '樂天桃猿':\n",
        "    win_ = win_ - 1\n",
        "  elif pitcher['win_team'][x] == '平手':\n",
        "    win_ = win_\n",
        "  else:\n",
        "    lose_ = lose_ - 1\n",
        "\n",
        "  if win_ + lose_ != 0:\n",
        "    pitcher['win_rate'][x] = win_ / (win_ + lose_)\n",
        "  else:\n",
        "    pitcher['win_rate'][x] = 0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M58Y326aIVnR",
        "outputId": "d78d2441-82b4-41db-ef0e-f29b9c8891bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-2248d1ea0c32>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  pitcher['win_rate'][x] = win_ / (win_ + lose_)\n",
            "<ipython-input-49-2248d1ea0c32>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  pitcher['weekend'][x] = 1\n",
            "<ipython-input-49-2248d1ea0c32>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  pitcher['win_rate'][x] = 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pitcher.to_csv('pitcher.csv', index = False)"
      ],
      "metadata": {
        "id": "9k5eIf83JenU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 天氣資料整理"
      ],
      "metadata": {
        "id": "Tjij4qC-6Y4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "date = [element.replace('/', '-') for element in date]\n",
        "Daydata_date, DayData_hour, DayData_Temperature, DayData_Precp, DayData_RH = [], [], [], [], []\n",
        "for element in date:\n",
        "  url = f'https://e-service.cwb.gov.tw/HistoryDataQuery/DayDataController.do?command=viewMain&station=C0C700&stname=%25E4%25B8%25AD%25E5%25A3%25A2&datepicker={element}&altitude=151m'\n",
        "  soup = crawler(url)\n",
        "  tbody = soup.find('tbody').find_all('tr')\n",
        "  for tr in tbody[3:]:\n",
        "    Daydata_date.append(element)\n",
        "    for_hour = tr.find_all('td')[0].text\n",
        "    DayData_hour.append(for_hour)\n",
        "\n",
        "    for_temp = tr.find_all('td')[3].text.strip('\\xa0')\n",
        "    DayData_Temperature.append(for_temp)\n",
        "\n",
        "    for_RH = tr.find_all('td')[5].text.strip('\\xa0')\n",
        "    DayData_RH.append(for_RH)\n",
        "\n",
        "    for_precp = tr.find_all('td')[10].text.strip('\\xa0')\n",
        "    DayData_Precp.append(for_precp)"
      ],
      "metadata": {
        "id": "xs__tcI9uW26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rakuten_home_hour_weather = pd.DataFrame({\n",
        "    'Daydata_date': Daydata_date,\n",
        "    'hour': DayData_hour,\n",
        "    'hour_Temperature': DayData_Temperature,\n",
        "    'hour_Precp': DayData_Precp,\n",
        "    'hour_RH': DayData_RH})\n",
        "\n",
        "rakuten_home_hour_weather.to_csv('rakuten_home_hour_weather.csv', index = False)"
      ],
      "metadata": {
        "id": "wW_vmldh0C1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MonthData_Temperature, MonthData_Precp, MonthData_RH = [], [], []\n",
        "for element in date:\n",
        "  year = element[:4]\n",
        "  month = element[5:7]\n",
        "  day = int(element[8:10])\n",
        "  url = f'https://e-service.cwb.gov.tw/HistoryDataQuery/MonthDataController.do?command=viewMain&station=C0C700&stname=%25E4%25B8%25AD%25E5%25A3%25A2&datepicker={year}-{month}&altitude=151m'\n",
        "  soup = crawler(url)\n",
        "  tbody = soup.find('tbody').find_all('tr')\n",
        "\n",
        "  for_temp = tbody[day+2].find_all('td')[7].text.strip('\\xa0')\n",
        "  MonthData_Temperature.append(for_temp)\n",
        "\n",
        "  for_RH = tbody[day+2].find_all('td')[13].text.strip('\\xa0')\n",
        "  MonthData_RH.append(for_RH)\n",
        "\n",
        "  for_prep = tbody[day+2].find_all('td')[21].text.strip('\\xa0')\n",
        "  MonthData_Precp.append(for_prep)"
      ],
      "metadata": {
        "id": "NZbpiT0z8CnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rakuten_home_day_weather = pd.DataFrame({\n",
        "    'date': date,\n",
        "    'date_Temperature': MonthData_Temperature,\n",
        "    'date_Precp': MonthData_Precp\n",
        "})\n",
        "\n",
        "rakuten_home_day_weather.to_csv('rakuten_home_day_weather.csv', index = False)"
      ],
      "metadata": {
        "id": "LB6hXyZQ7PZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RH_merge = []\n",
        "clean_data = pd.read_csv('audience_data.csv')\n",
        "for row in range(120):\n",
        "  play_hour = str(clean_data['hour'][row]).split('.')[0]\n",
        "  play_date = clean_data['date'][row]\n",
        "  Index=rakuten_home_hour_weather.index[(rakuten_home_hour_weather['Daydata_date']==play_date)&(rakuten_home_hour_weather['hour']==play_hour)].tolist()[0]\n",
        "  for_RH = rakuten_home_hour_weather['hour_RH'][Index]\n",
        "  RH_merge.append(for_RH)\n",
        "\n",
        "clean_data['hour_RH'] = RH_merge\n",
        "clean_data.to_csv('audience_data_2.csv', index = False)"
      ],
      "metadata": {
        "id": "R6-fS_IfQuN-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}